{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "M6CudZKwGMYm",
        "GYSHOlAzF8bY",
        "sjdE57CISS2a",
        "4WrxINu5J_gy",
        "-6cL6KatO6za",
        "o7B46FSyZc1A"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SandeshBashyal/naamii_labs_2023/blob/main/Lab_8_Visual_Odometry.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Before you start\n",
        "\n",
        "* Make sure to add shortcut to **Day8_Visual_Odometry** to your own drive\n",
        "* Make sure to make a copy of this file in your own drive.\n",
        "* You need to add codes where marked by **#TODO**\n",
        "\n",
        "# Reference Materials\n",
        "* [Multi View Geometry - Richard Hartley and Andrew Zisserman](https://users.cecs.anu.edu.au/~hartley/Papers/CVPR99-tutorial/tutorial.pdf)\n",
        "* [Slides on Multi View Geometry](https://users.cecs.anu.edu.au/~hartley/Papers/CVPR99-tutorial/tutorial.pdf)\n",
        "* [Introduction to Linear Algebra - Gilbert Strang](http://students.aiu.edu/submissions/profiles/resources/onlineBook/Y5B7M4_Introduction_to_Linear_Algebra-_Fourth_Edition.pdf)\n",
        "* [Probabilistic Robotics - Sebastian Thrun](https://docs.ufpr.br/~danielsantos/ProbabilisticRobotics.pdf)\n",
        "* [ORB_SLAM3](https://github.com/UZ-SLAMLab/ORB_SLAM3)"
      ],
      "metadata": {
        "id": "8NljIMXzbrcr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "M6CudZKwGMYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "import tqdm\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go"
      ],
      "metadata": {
        "id": "07xvM5uPF3SX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Dataset"
      ],
      "metadata": {
        "id": "GYSHOlAzF8bY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "id": "-nR1uvGmCh_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity Check\n",
        "try:\n",
        "  files = os.listdir('/gdrive/MyDrive/Day8_Visual_Odometry')\n",
        "  print(f\"Following files are available: {files}\")\n",
        "  print(\"You are good to go ðŸ™‚\")\n",
        "except:\n",
        "  print('Dataset is not available ðŸ˜Ÿ. Please check if there is a dir \"Day8_Visual_Odometry\" in your google drive.')"
      ],
      "metadata": {
        "id": "uOJirYnNaAPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract kitti dataset\n",
        "!mkdir kitti\n",
        "!tar -xJf /gdrive/MyDrive/SLAM_Lab_NAAMII/kitti.tar.xz -C kitti"
      ],
      "metadata": {
        "id": "N6wNwzKMcFUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the list of image paths\n",
        "frames = glob.glob(\"kitti/*.png\")\n",
        "frames.sort()\n",
        "\n",
        "if len(frames)==0:\n",
        "  print(\"There are no images ðŸ˜Ÿ! You must have messed up the PATH!!!\")\n",
        "else:\n",
        "  print(f\"{len(frames)} frames available! You are good to go ðŸ™‚\")"
      ],
      "metadata": {
        "id": "g0VujwDwHyQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility Functions"
      ],
      "metadata": {
        "id": "sjdE57CISS2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image(img):\n",
        "  fig = plt.gcf()\n",
        "  fig.set_size_inches(25,25)\n",
        "  plt.imshow(img)\n",
        "\n",
        "def estimate_relative_pose(extractor, frame_1, frame_2, cam_mat):\n",
        "    kpts_1, descs_1 = extractor.detectAndCompute(frame_1, None)\n",
        "    kpts_2, descs_2 = extractor.detectAndCompute(frame_2, None)\n",
        "\n",
        "    matches = matcher.knnMatch(descs_1, descs_2, k=2)\n",
        "\n",
        "    RATIO = 0.8\n",
        "    good = []\n",
        "    matched_kpts_1, matched_kpts_2 = [], []\n",
        "    for (m1, m2) in matches: # for every descriptor, take closest two matches\n",
        "        if m1.distance < RATIO * m2.distance: # best match has to be this much closer than second best\n",
        "            good.append(m1)\n",
        "            matched_kpts_1.append(kpts_1[m1.queryIdx].pt)\n",
        "            matched_kpts_2.append(kpts_2[m2.trainIdx].pt)\n",
        "    matched_kpts_1 = np.array(matched_kpts_1, dtype=np.float32)\n",
        "    matched_kpts_2 = np.array(matched_kpts_2, dtype=np.float32)\n",
        "\n",
        "    E, mask = cv2.findEssentialMat(\n",
        "    matched_kpts_2, matched_kpts_1,\n",
        "    cam_mat,\n",
        "    method=cv2.RANSAC, prob=0.999, threshold=1.0)\n",
        "    _, R, t, mask = cv2.recoverPose(E, matched_kpts_2, matched_kpts_1, cam_mat)\n",
        "\n",
        "    mask = (mask > 0)[..., 0]\n",
        "    matched_kpts_1 = matched_kpts_1[mask]\n",
        "    matched_kpts_2 = matched_kpts_2[mask]\n",
        "\n",
        "    return R, t, matched_kpts_1, matched_kpts_2\n",
        "\n",
        "def triangulate_3d_points(\n",
        "    points_1, points_2, \n",
        "    intrinsic_matrix, \n",
        "    R1, t1, R2, t2\n",
        "    ):\n",
        "    R_t_old = np.zeros([3,4], dtype=np.float32)\n",
        "    R_t_old[..., :-1] = R1\n",
        "    R_t_old[..., -1] = t1\n",
        "    cam_old = cam_mat @ R_t_old\n",
        "\n",
        "    R_t_new = np.zeros([3,4], dtype=np.float32)\n",
        "    R_t_new[..., :-1] = R2\n",
        "    R_t_new[..., -1] = t2\n",
        "    cam_new = cam_mat @ R_t_new\n",
        "\n",
        "    triangulated_points_homogeneous = cv2.triangulatePoints(\n",
        "        cam_old, cam_new, \n",
        "        points_1.T, points_2.T\n",
        "    ).T\n",
        "\n",
        "    triangulated_points = triangulated_points_homogeneous[..., :-1] / triangulated_points_homogeneous[..., [-1]]\n",
        "\n",
        "    distances = np.linalg.norm(triangulated_points - t1, axis=1)\n",
        "    triangulated_points = triangulated_points[distances < 10.0]\n",
        "\n",
        "    camera_axis = np.array([0,0,-1]).T\n",
        "    dots = (R_1 @ camera_axis).dot(triangulated_points.T)\n",
        "    mask_from_dots = dots > 0.0\n",
        "\n",
        "    return triangulated_points[mask_from_dots]\n",
        "\n",
        "def get_ground_truth_scales(ground_truth_scales_file):\n",
        "    scales = []\n",
        "    last_t = None\n",
        "    for line in ground_truth_scales_file:\n",
        "        Rt = np.fromstring(line, sep=' ')\n",
        "        Rt = Rt.reshape(3,4)\n",
        "        R = Rt[:, :3]\n",
        "        t = Rt[:, 3]\n",
        "        if last_t is not None:\n",
        "            scales.append(\n",
        "                np.linalg.norm(t - last_t)\n",
        "            )\n",
        "        last_t = t\n",
        "    return scales"
      ],
      "metadata": {
        "id": "bsHxDYufSRs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Feature Extraction"
      ],
      "metadata": {
        "id": "RkXR5tk_Gdhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a feature extractor\n",
        "feature_extractor = cv2.ORB_create(nfeatures=4096)"
      ],
      "metadata": {
        "id": "fODie39BGuQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read first frame\n",
        "frame_0 = cv2.imread(frames[0])\n",
        "\n",
        "# Convert to Grayscale\n",
        "frame_0 = cv2.cvtColor(frame_0, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Detect Keypoint in the frame with descriptors\n",
        "keypoints_0, descriptors_0 = feature_extractor.detectAndCompute(frame_0,None)\n",
        "\n",
        "# Draw Keypoints on the frame image\n",
        "frame_0_with_keypoints = cv2.drawKeypoints(frame_0, keypoints_0, None, flags=0)\n",
        "\n",
        "# Show the frame with keypoints\n",
        "show_image(frame_0_with_keypoints)"
      ],
      "metadata": {
        "id": "jPQKB0B7GbiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Feature Matching"
      ],
      "metadata": {
        "id": "4WrxINu5J_gy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# with the name image.jpg  \n",
        "# TODO: Read frames[3], similar to frames[0] above\n",
        "frame_3 = # ??\n",
        "\n",
        "# TODO: Get keypoints_3, descriptors_3  for frame_3\n",
        "keypoints_3, descriptors_3 = # ??\n",
        "\n",
        "  \n",
        "# Define Matcher\n",
        "matcher = cv2.BFMatcher()\n",
        "\n",
        "# Match keypoints in frame 1 and 3\n",
        "matches = matcher.knnMatch(descriptors_0, descriptors_3, k=2)\n",
        "\n",
        "# Apply Ratio Test\n",
        "# TODO: Try with ratios: 0.6. 0.7, 0.8, 0.9, 1\n",
        "RATIO = 0.8\n",
        "good_matches = []\n",
        "for (m1, m2) in matches: # for every descriptor, take closest two matches\n",
        "    if m1.distance < RATIO * m2.distance: # best match has to be this much closer than second best\n",
        "        good_matches.append(m1)\n",
        "\n",
        "good_matches = sorted(good_matches, key = lambda x:x.distance)\n",
        "   \n",
        "# Plot the matches\n",
        "image_with_matches = cv2.drawMatches(frame_0, keypoints_0, frame_3, keypoints_3, good_matches[:200], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
        "  \n",
        "# Show matches\n",
        "show_image(image_with_matches)"
      ],
      "metadata": {
        "id": "ht9GkZGnGKam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Visual Odometry"
      ],
      "metadata": {
        "id": "-6cL6KatO6za"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-defined camera intrinsics\n",
        "cam_mat = np.array([\n",
        "    [7.188560000000e+02, 0.000000000000e+00, 6.071928000000e+02],\n",
        "    [0.000000000000e+00, 7.188560000000e+02, 1.852157000000e+02],\n",
        "    [0.000000000000e+00, 0.000000000000e+00, 1.000000000000e+00],\n",
        "])\n",
        "\n",
        "# Load Ground truth scales\n",
        "gts = get_ground_truth_scales(open(\"/gdrive/MyDrive/Day8_Visual_Odometry/depths.txt\"))\n",
        "\n",
        "# Define number of images to use\n",
        "# TODO You can change the number of images, max=3000\n",
        "# Single frame can take up to 3 sec\n",
        "no_of_images = 50\n",
        "\n",
        "# Read initial frame\n",
        "frame_1 = cv2.imread(frames[0])\n",
        "\n",
        "# Initial Pose is identity\n",
        "R_id = np.eye(3)\n",
        "t_id = np.array([0,0,0], dtype=np.float32)\n",
        "\n",
        "R_1 = R_id.copy()\n",
        "t_1 = t_id.copy()\n",
        "\n",
        "# For the collect of Poses, Depths and 3D point clouds\n",
        "ts = []\n",
        "Rs = []\n",
        "dts = []\n",
        "point_cloud_map = []\n",
        "\n",
        "print(f\"Running Visual Odometry for {no_of_images} images...\")\n",
        "for image_file_name, ground_truth_scale in tqdm.tqdm(zip(frames[1:no_of_images], gts)):\n",
        "\n",
        "    # Get the next frame\n",
        "    frame_2 = #TODO read image from \"image_file_name\"\n",
        "\n",
        "    # TODO: Estimate relative pose between the two frames \n",
        "    # \"estimate_relative_pose\" function is available\n",
        "    # Parameters:\n",
        "    # 1. Feature Extractor\n",
        "    # 2. First Frame\n",
        "    # 3. Second Frame\n",
        "    # 4. Camera intrinsic matrix\n",
        "    dR, dt, points_1, points_2 = # ??\n",
        "\n",
        "    dt = dt[...,0] * ground_truth_scale\n",
        "    dts.append(dt)\n",
        "\n",
        "    # Get absolute pose of this frame\n",
        "    R_2 = dR.dot(R_1)\n",
        "    t_2 = t_1 + R_1.dot(dt)\n",
        "\n",
        "    # TODO: Triangulation\n",
        "    # Function \"triangulate_3d_points\" is provided\n",
        "    # Parameters: \n",
        "    # 1. Points from first image\n",
        "    # 2. Points from second image\n",
        "    # 3. Camera intrinsic matrix\n",
        "    # 4. Identity rotation matrix\n",
        "    # 5. Identity translation vector\n",
        "    # 6. Relative rotation matrix\n",
        "    # 7. Relative translation vector\n",
        "    triangulated_points = # ??\n",
        "\n",
        "\n",
        "    triangulated_points = (R_2 @ triangulated_points.T).T\n",
        "    triangulated_points += t_2\n",
        "\n",
        "    # Collect the Poses and 3D point clouds\n",
        "    point_cloud_map.append(triangulated_points)\n",
        "    ts.append(t_2)\n",
        "    Rs.append(R_2)\n",
        "\n",
        "    # Prepare for next loop\n",
        "    t_1 = t_2\n",
        "    R_1 = R_2\n",
        "    frame_1 = frame_2\n",
        "\n",
        "ts = np.array(ts)\n",
        "dts = np.array(dts)\n",
        "point_cloud_map_concatted = np.concatenate(point_cloud_map)\n",
        "\n",
        "print(f\"{point_cloud_map_concatted.shape[0]} 3D points has been triangulated.\")"
      ],
      "metadata": {
        "id": "FEreiYRNMSQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Visualization"
      ],
      "metadata": {
        "id": "o7B46FSyZc1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig1 = px.scatter_3d(point_cloud_map_concatted, x=0, y=2, z=1)\n",
        "fig1.update_traces(marker={'size': 1})\n",
        "\n",
        "camera_axes = [R.T @ np.array([0,0,-1]).T for R in Rs] \n",
        "camera_axes = np.array(camera_axes)\n",
        "\n",
        "fig2 = go.Figure(data=go.Cone(\n",
        "    x=ts[..., 0],\n",
        "    y=ts[..., 2],\n",
        "    z=ts[..., 1],\n",
        "    u=dts[..., 0],\n",
        "    v=dts[..., 2],\n",
        "    w=dts[..., 1],\n",
        "    sizemode=\"absolute\",\n",
        "    sizeref=3,\n",
        "    anchor=\"tail\"))\n",
        "\n",
        "go.Figure(data=fig1.data + fig2.data).show()"
      ],
      "metadata": {
        "id": "ChRmbKePM6fV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FSXbCdPXXE3a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}